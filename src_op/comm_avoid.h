
#ifndef COMM_AVOID_H
#define COMM_AVOID_H
#ifdef COMM_AVOID

#include ".././src/Kernels/test_write.h"

// host stub function
void ca_loop_test_write_kernel(char const *name, op_set set,
  op_arg arg0,
  op_arg arg1,
  int n_lower, int n_upper){

  int nargs = 2;
  op_arg args[2];

  args[0] = arg0;
  args[1] = arg1;

  // initialise timers
  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  op_timing_realloc(25);
  op_timers_core(&cpu_t1, &wall_t1);

  if (OP_diags>2) {
    printf(" kernel routine with indirection: test_write_kernel\n");
  }

  // int set_size = op_mpi_halo_exchanges(set, nargs, args);
  // op_mpi_wait_all(nargs, args);

    for ( int n=n_lower; n<n_upper; n++ ){
        int map0idx;
        int map1idx;
        map0idx = arg0.map_data[n * arg0.map->dim + 0];
        map1idx = arg0.map_data[n * arg0.map->dim + 1];


        test_write_kernel(
        &((double*)arg0.data)[5 * map0idx],
        &((double*)arg0.data)[5 * map1idx]);
    }

  // update kernel record
  op_timers_core(&cpu_t2, &wall_t2);
  OP_kernels[25].name      = name;
  OP_kernels[25].count    += 1;
  OP_kernels[25].time     += wall_t2 - wall_t1;
  OP_kernels[25].transfer += (float)set->size * arg0.size * 2.0f;
  OP_kernels[25].transfer += (float)set->size * arg0.map->dim * 4.0f;
}


//
// auto-generated by op2.py
//

//user function
#include ".././src/Kernels/test_read.h"

// host stub function
void ca_par_loop_test_read_kernel(char const *name, op_set set,
  op_arg arg0,
  op_arg arg1,
  op_arg arg2,
  op_arg arg3,
  op_arg arg4,
  int n_lower, int n_upper){

  int nargs = 5;
  op_arg args[5];

  args[0] = arg0;
  args[1] = arg1;
  args[2] = arg2;
  args[3] = arg3;
  args[4] = arg4;

  // initialise timers
  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  op_timing_realloc(26);
  op_timers_core(&cpu_t1, &wall_t1);

  if (OP_diags>2) {
    printf(" kernel routine with indirection: test_read_kernel\n");
  }

  // op_mpi_halo_exchanges_chained(set, nargs, args, 1, 1);
  // op_mpi_wait_all_chained(nargs, args, 1);
  // int set_size = op_mpi_halo_exchanges(set, nargs, args);
  // op_mpi_wait_all(nargs, args);

  for ( int n=n_lower; n<n_upper; n++ ){
    int map0idx;
    int map1idx;
    map0idx = arg0.map_data[n * arg0.map->dim + 0];
    map1idx = arg0.map_data[n * arg0.map->dim + 1];


    test_read_kernel(
      &((double*)arg0.data)[5 * map0idx],
      &((double*)arg0.data)[5 * map1idx],
      &((double*)arg2.data)[3 * n],
      &((double*)arg3.data)[5 * map0idx],
      &((double*)arg3.data)[5 * map1idx]);
  }
  
  // update kernel record
  op_timers_core(&cpu_t2, &wall_t2);
  OP_kernels[26].name      = name;
  OP_kernels[26].count    += 1;
  OP_kernels[26].time     += wall_t2 - wall_t1;
  OP_kernels[26].transfer += (float)set->size * arg0.size;
  OP_kernels[26].transfer += (float)set->size * arg3.size * 2.0f;
  OP_kernels[26].transfer += (float)set->size * arg2.size;
  OP_kernels[26].transfer += (float)set->size * arg0.map->dim * 4.0f;
}


#include "op_lib_mpi.h"
#include "op_mpi_core.h"

void test_comm_avoid(char const *name, op_dat* p_variables, op_dat p_edge_weights, op_dat p_fluxes, op_map p_edge_to_nodes, op_set set,
                      int nloops, int nchains, int default_variable_index){
   
    int nhalos = nloops; // * nchains;
    int map_index = nhalos;

#ifdef SINGLE_DAT_VAR
    int nargs_ex0 = 1;
    op_arg args_ex0[1];

    for(int i = 0; i < 1; i++){
#else
    int nargs_ex0 = nchains;
    op_arg args_ex0[nchains];

    for(int i = 0; i < nchains; i++){
#endif
      args_ex0[i] = op_arg_dat_halo(p_variables[i],0,p_edge_to_nodes,5,"double",OP_READ, nhalos, map_index);
      set_dat_dirty(&args_ex0[i]);
    }

    double cpu_t1, cpu_t2, wall_t1, wall_t2;
    op_timing_realloc(28);
    op_timers_core(&cpu_t1, &wall_t1);
    OP_kern_curr = 28;
    op_mpi_halo_exchanges_chained(set, nargs_ex0, args_ex0, nhalos, 1);

    int nargs0 = 2;
    op_arg args0[nchains][2];

    int nargs1 = 5;
    op_arg args1[nchains][5];

    for(int i = 0; i < nchains; i++){
#ifdef SINGLE_DAT_VAR
      int var_index = 0;
#else
      int var_index = i;
#endif
      args0[i][0] = op_arg_dat_halo(p_variables[var_index],0,p_edge_to_nodes,5,"double",OP_INC, nhalos, map_index);
      args0[i][1] = op_arg_dat_halo(p_variables[var_index],1,p_edge_to_nodes,5,"double",OP_INC, nhalos, map_index);

      args1[i][0] = op_arg_dat_halo(p_variables[var_index],0,p_edge_to_nodes,5,"double",OP_READ, nhalos, map_index);
      args1[i][1] = op_arg_dat_halo(p_variables[var_index],1,p_edge_to_nodes,5,"double",OP_READ, nhalos, map_index);
      args1[i][2] = op_arg_dat(p_edge_weights,-1,OP_ID,3,"double",OP_READ);
      args1[i][3] = op_arg_dat_halo(p_fluxes,0,p_edge_to_nodes,5,"double",OP_INC, nhalos, map_index);
      args1[i][4] = op_arg_dat_halo(p_fluxes,1,p_edge_to_nodes,5,"double",OP_INC, nhalos, map_index);
    }

    // this will do the latency hiding for the first two loops in the chain, since halo extension is done for two levels.
    // other levels will receive 0 as core size.
#ifdef SINGLE_DAT_VAR
      // when using a single dat we can do latency hiding, if we reduce the core size gradually in the loop chain.
      // if we have nchains = 2, core sizes should be 1, 2, 3, 4 when the halo extension goes from 4, 3, 2, 1.
      // we have to make sure the OP2 CA backend calulates corre sizes for these levels.
      // if we consider core sizes as 1, 2, we cannot do latency hiding for the single dat version.
      int n_lower0 = 0;
      int n_lower1 = 0;
#else
      int n_lower0 = get_set_core_size(set, nloops - 1);
      int n_lower1 = get_set_core_size(set, nloops);
#endif
    

    for(int i = 0; i < nchains; i++){

#ifdef SINGLE_DAT_VAR
      n_lower0 = get_set_core_size(set, i * nloops + 1);
      n_lower1 = get_set_core_size(set, i * nloops + 2);
#endif
      ca_loop_test_write_kernel("ca_test_write_kernel",set,
                            args0[i][0], args0[i][1], 0, n_lower0);

      ca_par_loop_test_read_kernel("ca_test_read_kernel",set,
                            args1[i][0], args1[i][1], args1[i][2], args1[i][3], args1[i][4], 0, n_lower1);
    }

    OP_kern_curr = 28;
    op_mpi_wait_all_chained(nargs_ex0, args_ex0, 1);

    int n_upper0 = 0;
    int n_upper1 = 0;
    
    for(int i = 0; i < nchains; i++){

#ifdef SINGLE_DAT_VAR
      // n_lower0 = 0;
      n_lower0 = get_set_core_size(set, i * nloops + 1);
#else
      n_lower0 = get_set_core_size(set, nloops - 1);
#endif
      n_upper0 = set->size;
      ca_loop_test_write_kernel("ca_test_write_kernel",set,
                            args0[i][0], args0[i][1], n_lower0, n_upper0);

      for(int j = 1; j <= nloops; j++){
        n_lower0 = get_halo_start_size(set, j);
        n_upper0 = get_halo_end_size(set, j);
        ca_loop_test_write_kernel("ca_test_write_kernel",set,
                            args0[i][0], args0[i][1], n_lower0, n_upper0);
      }

#ifdef SINGLE_DAT_VAR
      // n_lower1 =0;
      n_lower1 = get_set_core_size(set, i * nloops + 2);
#else
      n_lower1 = get_set_core_size(set, nloops);
#endif
      n_upper1 = set->size;

      ca_par_loop_test_read_kernel("ca_test_read_kernel",set,
                            args1[i][0], args1[i][1], args1[i][2], args1[i][3], args1[i][4], n_lower1, n_upper1);
      
      for(int j = 1; j <= nloops - 1; j++){
        n_lower1 = get_halo_start_size(set, j);
        n_upper1 = get_halo_end_size(set, j);
        ca_par_loop_test_read_kernel("ca_test_read_kernel",set,
                            args1[i][0], args1[i][1], args1[i][2], args1[i][3], args1[i][4], n_lower1, n_upper1);
      }
    }

    // op_mpi_set_dirtybit(nargs0, args0);
    op_mpi_set_dirtybit(nargs1, args1[default_variable_index]);

    op_timers_core(&cpu_t2, &wall_t2);
    OP_kernels[28].name      = name;
    OP_kernels[28].count    += 1;
    OP_kernels[28].time     += wall_t2 - wall_t1;
    OP_kernels[28].transfer += (float)set->size * args_ex0[default_variable_index].size;
    OP_kernels[28].transfer += (float)set->size * args_ex0[default_variable_index].size * 2.0f;
    OP_kernels[28].transfer += (float)set->size * args_ex0[default_variable_index].size;
    OP_kernels[28].transfer += (float)set->size * args_ex0[default_variable_index].map->dim * 4.0f;
   
}
#endif
#endif